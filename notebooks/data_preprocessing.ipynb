{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz4jDwJ7Vpom"
   },
   "source": [
    "## **Necessary Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0bTp2nGtbHQb"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style='whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hq9R3knHVsZ-"
   },
   "source": [
    "## **Mounting Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_lVlyvRaqNw",
    "outputId": "d7ff0bc5-1fd5-4514-f0df-3b3a207be068"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Index        F1        F2        F3        F4    F5    F6    F7    F8  \\\n",
       " 0      1  0.224506  0.500340  0.489860  0.902413  7934 -6970 -5714  9982   \n",
       " 1      2  0.321128  0.281119  0.907283  0.772159 -8238  1219  1663  1287   \n",
       " 2      3  0.893441  0.622005  0.998776  0.098386  8540  5266 -9377 -3504   \n",
       " 3      4  0.320641  0.957234  0.346000  0.646479 -7772  -383  9681 -8661   \n",
       " 4      5  0.475961  0.623008  0.544988  0.159709  1571 -8039 -7961 -2385   \n",
       " \n",
       "      F9  ...         F14        F15         F16  F17  F18  F19  F20  F21  F22  \\\n",
       " 0 -5697  ... -3433637453  10/4/1986    9/6/1992    2    1  706  305    1    2   \n",
       " 1 -3658  ...   609277486  2/24/1979    1/5/1983    1    1  423  206   18    7   \n",
       " 2 -4511  ... -8977995005  1/12/1989  11/22/1986    2    1  703  315    1    4   \n",
       " 3  3474  ...  4868760308  2/18/1982   6/10/1992    1    1  122  304   15    1   \n",
       " 4  4407  ...  9757408267  4/10/1987  10/19/1985    1    1  486  240    1    1   \n",
       " \n",
       "    C  \n",
       " 0  0  \n",
       " 1  1  \n",
       " 2  0  \n",
       " 3  0  \n",
       " 4  0  \n",
       " \n",
       " [5 rows x 24 columns],\n",
       "        Index        F1        F2        F3        F4    F5    F6    F7    F8  \\\n",
       " 0  T30234341  0.654765  0.812009  0.603190  0.391039 -5220  4825 -1784  7447   \n",
       " 1  T30234342  0.694636  0.690568  0.473460  0.259760  -618 -5018  2012  9259   \n",
       " 2  T30234343  0.203759  0.323301  0.492294  0.011448 -8778  6141  6965  3774   \n",
       " 3  T30234344  0.319627  0.286247  0.906197  0.093840 -7929  4471  7715  9543   \n",
       " 4  T30234345  0.236003  0.782784  0.285689  0.383585 -3296  4564 -1580 -8559   \n",
       " \n",
       "      F9  ...         F13         F14        F15        F16  F17 F18  F19  F20  \\\n",
       " 0 -7147  ...  8074343777  5553595074  9/17/1996  8/18/1990    1   1  436  478   \n",
       " 1  9267  ... -5556861821  2216284070  11/7/1985  4/11/1990    1   1  138   56   \n",
       " 2  4303  ... -2121815725  -315409510   7/9/1984   5/4/1997    1   1  117  323   \n",
       " 3   335  ...  1389754605 -3360224957   6/3/1987   7/1/1988    1   1  115  149   \n",
       " 4   -27  ...  3653338555  7604838279  7/17/1984  12/3/1993    1   1  527  281   \n",
       " \n",
       "    F21  F22  \n",
       " 0    1    1  \n",
       " 1   10    4  \n",
       " 2   10    1  \n",
       " 3   16   21  \n",
       " 4    3    1  \n",
       " \n",
       " [5 rows x 23 columns])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training and test datasets from your local system\n",
    "train_data = pd.read_csv(r'C:\\Users\\adity\\price-comparison-project\\Dataset.txt', sep='\\t')\n",
    "test_data = pd.read_csv(r'C:\\Users\\adity\\price-comparison-project\\Dataset_test.txt', sep='\\t')\n",
    "\n",
    "# Display the first few rows to check the data\n",
    "train_data.head(), test_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEAu0TrAVusl"
   },
   "source": [
    "## **Initalizing data folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0pSOiRJare0",
    "outputId": "2081c46b-4eb6-4eeb-e800-395c02796047"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with the actual path to the Dataset.txt file\n",
    "train_data = pd.read_csv('C:/Users/adity/price-comparison-project/Dataset.txt', sep='\\t')\n",
    "test_data = pd.read_csv('C:/Users/adity/price-comparison-project/Dataset_test.txt', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZy1n6WpWKVU"
   },
   "source": [
    "## **Assumption: - As we have to predict the customer purhase behavior based on the given features, so, need to find out the relevant customer characteristics while from the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfyEntKB1Bzo"
   },
   "source": [
    "## **Dataset Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JR-mWImdVzbu"
   },
   "source": [
    "### **Reading csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RFt2t-jIariV"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"C:/Users/adity/price-comparison-project/Dataset.txt\", sep='\\t', encoding=\"unicode_escape\")\n",
    "df_test = pd.read_csv(\"C:/Users/adity/price-comparison-project/Dataset_test.txt\", sep='\\t', encoding=\"unicode_escape\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y42ZSNIBV1rF"
   },
   "source": [
    "### **Viewing dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "Lt3oVIcRarlt",
    "outputId": "08203b38-2edc-4707-dff8-0011af4bff56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Index', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10',\n",
      "       'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20',\n",
      "       'F21', 'F22', 'C'],\n",
      "      dtype='object')\n",
      "Index(['Index', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10',\n",
      "       'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20',\n",
      "       'F21', 'F22'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_train.columns)  # Should contain 'C'\n",
    "print(df_test.columns)   # Should not contain 'C'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7AFv5kqxMhQ"
   },
   "source": [
    "### **Number of rows and columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oi-SmMJMxERQ",
    "outputId": "d94545b8-db1c-4b05-b73a-63ea233c058d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns: (101180, 24)\n"
     ]
    }
   ],
   "source": [
    "# Display the number of rows and columns\n",
    "print(f\"Number of rows and columns: {df_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQvfvIh5v7WD"
   },
   "source": [
    "### **Checking if NaN value exists.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYKoT8nQOVqc",
    "outputId": "ef99bde3-8ac6-4ed1-bb40-7f7c0205978e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if NaN values exist:\n",
      "Index    0\n",
      "F1       0\n",
      "F2       0\n",
      "F3       0\n",
      "F4       0\n",
      "F5       0\n",
      "F6       0\n",
      "F7       0\n",
      "F8       0\n",
      "F9       0\n",
      "F10      0\n",
      "F11      0\n",
      "F12      0\n",
      "F13      0\n",
      "F14      0\n",
      "F15      0\n",
      "F16      0\n",
      "F17      0\n",
      "F18      0\n",
      "F19      0\n",
      "F20      0\n",
      "F21      0\n",
      "F22      0\n",
      "C        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if NaN values exist in the dataset\n",
    "print(\"Checking if NaN values exist:\")\n",
    "print(df_train.isna().sum())  # This will show the count of NaN values per column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNZcDp8wwH86"
   },
   "source": [
    "#### **We got NaN values in the 'CustomerID'. As 'CustomerID' is the unique identifier for each customer, so, having NaN in this column won't let us know the customer. Therefore these rows should be deleted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bUSH93mw8Ka"
   },
   "source": [
    "### **Dropping NaN value columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bsCEntq9OpPC"
   },
   "outputs": [],
   "source": [
    "# Drop columns that contain NaN values\n",
    "df_train_clean = df_train.dropna(axis=1)  # This will remove columns with NaNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzHP16vtxrOn"
   },
   "source": [
    "### **No NaN values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VwgYUwl2xbCI",
    "outputId": "3ca1af52-24d8-406f-c123-efa2dab0931c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if NaN values remain after dropping columns/rows:\n",
      "Index    0\n",
      "F1       0\n",
      "F2       0\n",
      "F3       0\n",
      "F4       0\n",
      "F5       0\n",
      "F6       0\n",
      "F7       0\n",
      "F8       0\n",
      "F9       0\n",
      "F10      0\n",
      "F11      0\n",
      "F12      0\n",
      "F13      0\n",
      "F14      0\n",
      "F15      0\n",
      "F16      0\n",
      "F17      0\n",
      "F18      0\n",
      "F19      0\n",
      "F20      0\n",
      "F21      0\n",
      "F22      0\n",
      "C        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if any NaN values remain after dropping columns or rows\n",
    "print(\"Checking if NaN values remain after dropping columns/rows:\")\n",
    "print(df_train_clean.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTwfeTk-xwwf"
   },
   "source": [
    "### **Dataframe shape after dropping the NaN value rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5UHU-IVxzHX",
    "outputId": "852b8d01-bbc2-4685-d09b-dd03d74fe9f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape after dropping NaN rows: (101180, 24)\n"
     ]
    }
   ],
   "source": [
    "df_train_clean_rows = df_train.dropna(axis=0)  # Drop rows with NaN values\n",
    "print(f\"DataFrame shape after dropping NaN rows: {df_train_clean_rows.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxAe6cy6FwL3"
   },
   "source": [
    "### **Dataframe columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lu1v0q6cFu1n",
    "outputId": "86f534a2-1dda-4937-c331-a8d1adbc543b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame columns: Index(['Index', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10',\n",
      "       'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20',\n",
      "       'F21', 'F22', 'C'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display the column names of the DataFrame\n",
    "print(f\"DataFrame columns: {df_train.columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4ySxaSnyCTf"
   },
   "source": [
    "### **Checking dataframe column datatypes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6-FHNYmO4v5",
    "outputId": "ed93894e-37cd-4c1c-86c4-b52b707ff834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame column data types:\n",
      "Index      int64\n",
      "F1       float64\n",
      "F2       float64\n",
      "F3       float64\n",
      "F4       float64\n",
      "F5         int64\n",
      "F6         int64\n",
      "F7         int64\n",
      "F8         int64\n",
      "F9         int64\n",
      "F10        int64\n",
      "F11        int64\n",
      "F12        int64\n",
      "F13        int64\n",
      "F14        int64\n",
      "F15       object\n",
      "F16       object\n",
      "F17        int64\n",
      "F18        int64\n",
      "F19        int64\n",
      "F20        int64\n",
      "F21        int64\n",
      "F22        int64\n",
      "C          int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Display data types of each column\n",
    "print(\"DataFrame column data types:\")\n",
    "print(df_train.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRhM1FBQyLo2"
   },
   "source": [
    "### **The 'CustomerID' is in float64 format. So, this should be handled.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-YhohOsPDMZ",
    "outputId": "4907850b-3fdd-4453-8d9f-d569e9176972"
   },
   "outputs": [],
   "source": [
    "# Convert a valid column to int64 (example: 'Index' column)\n",
    "df_train['Index'] = df_train['Index'].astype('Int64')  # Use 'Int64' to retain NaN values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aQLZpNGHU4a"
   },
   "source": [
    "### **Exploring columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGsA18K0G-VD",
    "outputId": "0c4e39e9-8c69-4077-b726-40d4e36169ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring columns:\n",
      "Unique values in Index: 101180\n",
      "Unique values in F1: 101176\n",
      "Unique values in F2: 101169\n",
      "Unique values in F3: 101179\n",
      "Unique values in F4: 101178\n",
      "Unique values in F5: 19876\n",
      "Unique values in F6: 19862\n",
      "Unique values in F7: 19865\n",
      "Unique values in F8: 19866\n",
      "Unique values in F9: 19887\n",
      "Unique values in F10: 101179\n",
      "Unique values in F11: 101180\n",
      "Unique values in F12: 101180\n",
      "Unique values in F13: 101178\n",
      "Unique values in F14: 101180\n",
      "Unique values in F15: 8031\n",
      "Unique values in F16: 6300\n",
      "Unique values in F17: 5\n",
      "Unique values in F18: 5\n",
      "Unique values in F19: 646\n",
      "Unique values in F20: 646\n",
      "Unique values in F21: 21\n",
      "Unique values in F22: 21\n",
      "Unique values in C: 2\n"
     ]
    }
   ],
   "source": [
    "# Explore unique values in the columns\n",
    "print(\"Exploring columns:\")\n",
    "for column in df_train.columns:\n",
    "    print(f\"Unique values in {column}: {df_train[column].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70sSqzaR0YOB"
   },
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FG9qZhyEH5cK"
   },
   "source": [
    "### **Each customer bought different number of products for the unit price. So, we can calculate a new column 'Revenue_given' that tells how much the customer has spent ordering products.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "YJToziiwPawn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Index', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10',\n",
      "       'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20',\n",
      "       'F21', 'F22', 'C'],\n",
      "      dtype='object')\n",
      "NaN values after feature engineering: Index            0\n",
      "F1               0\n",
      "F2               0\n",
      "F3               0\n",
      "F4               0\n",
      "F5               0\n",
      "F6               0\n",
      "F7               0\n",
      "F8               0\n",
      "F9               0\n",
      "F10              0\n",
      "F11              0\n",
      "F12              0\n",
      "F13              0\n",
      "F14              0\n",
      "F15              0\n",
      "F16              0\n",
      "F17              0\n",
      "F18              0\n",
      "F19              0\n",
      "F20              0\n",
      "F21              0\n",
      "F22              0\n",
      "C                0\n",
      "Revenue_given    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you already have df_train loaded with the appropriate dataset\n",
    "\n",
    "# Drop NaN columns if any (removes columns that have NaN values)\n",
    "df_train_clean_cols = df_train.dropna(axis=1)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_train_clean_rows = df_train_clean_cols.dropna(axis=0)\n",
    "\n",
    "# Check the columns of the dataframe\n",
    "print(df_train_clean_rows.columns)\n",
    "\n",
    "# Proceed with feature engineering steps like calculating 'Revenue_given'\n",
    "# Since we don't have 'UnitPrice' or 'Quantity', adjust this based on the correct column names\n",
    "# Example assuming F1 is Price and F2 is Quantity - adjust as needed for your dataset\n",
    "df_train_clean_rows['Revenue_given'] = df_train_clean_rows['F1'] * df_train_clean_rows['F2']\n",
    "\n",
    "# Checking if there are any NaN values after new feature creation\n",
    "print(f\"NaN values after feature engineering: {df_train_clean_rows.isnull().sum()}\")\n",
    "\n",
    "# Now you can perform additional steps like frequency counting, country merging, etc.\n",
    "# Continue with other feature engineering steps like country frequency, handling outliers, etc.\n",
    "\n",
    "# Renaming columns if needed\n",
    "df_train_clean_rows.rename(columns={'F1': 'UnitPrice', 'F2': 'Quantity'}, inplace=True)\n",
    "\n",
    "# Checking for outliers and handling them\n",
    "# You may use z-score, IQR methods, or domain-specific thresholds to handle outliers\n",
    "\n",
    "# Exporting the processed dataset\n",
    "df_train_clean_rows.to_csv('processed_customer_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwI1drtdJ5Wu"
   },
   "source": [
    "### **Revenue Count for each customer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FpDPxAX4J-F6",
    "outputId": "f249b031-ed03-43f6-bf4f-0707a7246301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         F1  Revenue_given\n",
      "0  0.000018       0.484164\n",
      "1  0.000022       0.705824\n",
      "2  0.000033       0.237354\n",
      "3  0.000038       0.501637\n",
      "4  0.000048       0.134999\n"
     ]
    }
   ],
   "source": [
    "# First, ensure that 'Revenue_given' is correctly calculated\n",
    "# Assuming that 'UnitPrice' and 'Quantity' columns exist and can be used to calculate 'Revenue_given'\n",
    "df_train['Revenue_given'] = df_train['F2'] * df_train['F3']  # Assuming F2 is 'UnitPrice' and F3 is 'Quantity'\n",
    "\n",
    "# Group by the customer column (e.g., F1) and sum the 'Revenue_given' for each customer\n",
    "revenue_count = df_train.groupby('F1')['Revenue_given'].sum().reset_index()\n",
    "\n",
    "# Display the revenue count\n",
    "print(revenue_count.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v7T6pMAKhed"
   },
   "source": [
    "#### **We get revenue given to the company for each customer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veCgRgXtK13k"
   },
   "source": [
    "### **Let's find out how frequent the customer bought products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gQksQRihKrs8",
    "outputId": "a97e3516-65dd-4c0e-b4a5-289c33d9cd7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         F1  Purchase_Frequency\n",
      "0  0.000018                   1\n",
      "1  0.000022                   1\n",
      "2  0.000033                   1\n",
      "3  0.000038                   1\n",
      "4  0.000048                   1\n"
     ]
    }
   ],
   "source": [
    "frequency_count = df_train.groupby('F1').size().reset_index(name='Purchase_Frequency')\n",
    "\n",
    "# Display the frequency count\n",
    "print(frequency_count.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZE3H5b_Laur"
   },
   "source": [
    "#### **We get frequency count for each customer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB9fp5IRQ5Vg"
   },
   "source": [
    "### **For the same customer we get 2 countries with same amount of frequency. This is problematic. But as this is an important feature, so, we will take only one country and move with it.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_WQzyrwRjgW"
   },
   "source": [
    "### **Now we get country for each of the customer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "D7bzNtM7NE6z"
   },
   "outputs": [],
   "source": [
    "countries_df = new_df.drop([\"InvoiceNo\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wby-XKszSZ3F"
   },
   "source": [
    "## **Renaming some columns for clearance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xZh0vqyBR-iV"
   },
   "outputs": [],
   "source": [
    "frequency_count.columns =['CustomerID', 'Frequency']\n",
    "recency_count.columns = ['CustomerID', 'Recency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-cSPxZpSexV"
   },
   "source": [
    "## **Merging and creating a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xiNeUse7Thv6",
    "outputId": "97a7fd0f-94b9-48b7-9458-222681c6cc7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Index        F1        F2        F3        F4    F5    F6    F7    F8  \\\n",
      "0      1  0.224506  0.500340  0.489860  0.902413  7934 -6970 -5714  9982   \n",
      "1      2  0.321128  0.281119  0.907283  0.772159 -8238  1219  1663  1287   \n",
      "2      3  0.893441  0.622005  0.998776  0.098386  8540  5266 -9377 -3504   \n",
      "3      4  0.320641  0.957234  0.346000  0.646479 -7772  -383  9681 -8661   \n",
      "4      5  0.475961  0.623008  0.544988  0.159709  1571 -8039 -7961 -2385   \n",
      "\n",
      "     F9  ...         F16  F17  F18  F19  F20 F21 F22  C  Revenue_given_x  \\\n",
      "0 -5697  ...    9/6/1992    2    1  706  305   1   2  0         0.245097   \n",
      "1 -3658  ...    1/5/1983    1    1  423  206  18   7  1         0.255054   \n",
      "2 -4511  ...  11/22/1986    2    1  703  315   1   4  0         0.621244   \n",
      "3  3474  ...   6/10/1992    1    1  122  304  15   1  0         0.331203   \n",
      "4  4407  ...  10/19/1985    1    1  486  240   1   1  0         0.339532   \n",
      "\n",
      "   Revenue_given_y  \n",
      "0         0.245097  \n",
      "1         0.255054  \n",
      "2         0.621244  \n",
      "3         0.331203  \n",
      "4         0.339532  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge df_train with revenue_count on 'F1' column\n",
    "df_merged = pd.merge(df_train, revenue_count, on='F1', how='left')\n",
    "\n",
    "# Check the first few rows of the merged dataframe\n",
    "print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qgudy6l21089"
   },
   "source": [
    "### **Let's see if outliers exists in the numeric columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "2B_wf7rL11Rl",
    "outputId": "24f959fc-6098-47c2-c9ff-1a641d02ef7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train shape: (101180, 25), Cleaned train shape: (39025, 25)\n",
      "Original test shape: (19913, 23), Cleaned test shape: (11878, 23)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate IQR for numeric columns in df_train\n",
    "numeric_columns_train = df_train.select_dtypes(include=[np.number]).columns\n",
    "Q1_train = df_train[numeric_columns_train].quantile(0.25)\n",
    "Q3_train = df_train[numeric_columns_train].quantile(0.75)\n",
    "IQR_train = Q3_train - Q1_train\n",
    "outliers_train = ((df_train[numeric_columns_train] < (Q1_train - 1.5 * IQR_train)) | (df_train[numeric_columns_train] > (Q3_train + 1.5 * IQR_train)))\n",
    "\n",
    "# Remove rows with any outliers from df_train\n",
    "df_train_cleaned = df_train[~outliers_train.any(axis=1)]\n",
    "\n",
    "# Calculate IQR for numeric columns in df_test (similarly)\n",
    "numeric_columns_test = df_test.select_dtypes(include=[np.number]).columns\n",
    "Q1_test = df_test[numeric_columns_test].quantile(0.25)\n",
    "Q3_test = df_test[numeric_columns_test].quantile(0.75)\n",
    "IQR_test = Q3_test - Q1_test\n",
    "outliers_test = ((df_test[numeric_columns_test] < (Q1_test - 1.5 * IQR_test)) | (df_test[numeric_columns_test] > (Q3_test + 1.5 * IQR_test)))\n",
    "\n",
    "# Remove rows with any outliers from df_test\n",
    "df_test_cleaned = df_test[~outliers_test.any(axis=1)]\n",
    "\n",
    "# Check the shape of the cleaned data\n",
    "print(f\"Original train shape: {df_train.shape}, Cleaned train shape: {df_train_cleaned.shape}\")\n",
    "print(f\"Original test shape: {df_test.shape}, Cleaned test shape: {df_test_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S713k5tw11VN"
   },
   "source": [
    "#### **We can see that outliers exists in the numeric columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0uc721U11YV"
   },
   "source": [
    "### **Handling Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "N1lsmlT111cV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Shape: (101180, 24), Cleaned Train Shape: (60544, 24)\n",
      "Original Test Shape: (19913, 23), Cleaned Test Shape: (11878, 23)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv(r'C:\\Users\\adity\\price-comparison-project\\Dataset.txt', sep='\\t')\n",
    "df_test = pd.read_csv(r'C:\\Users\\adity\\price-comparison-project\\Dataset_test.txt', sep='\\t')\n",
    "\n",
    "# Step 1: Identify numeric columns in both datasets\n",
    "numeric_columns_train = df_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_columns_test = df_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Step 2: Align the columns (keep only those columns that are common in both datasets)\n",
    "common_columns = list(set(numeric_columns_train) & set(numeric_columns_test))\n",
    "\n",
    "# If no common columns exist, handle the case (e.g., raise an exception)\n",
    "if not common_columns:\n",
    "    raise ValueError(\"No common numeric columns found between the training and test datasets.\")\n",
    "\n",
    "# Step 3: Calculate Q1 (25th percentile) and Q3 (75th percentile) for each numeric column in df_train\n",
    "Q1 = df_train[common_columns].quantile(0.25)\n",
    "Q3 = df_train[common_columns].quantile(0.75)\n",
    "\n",
    "# Step 4: Calculate IQR (Interquartile Range)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Step 5: Identify outliers in df_train and df_test (apply outlier detection to common numeric columns)\n",
    "outliers_train = ((df_train[common_columns] < (Q1 - 1.5 * IQR)) | (df_train[common_columns] > (Q3 + 1.5 * IQR)))\n",
    "outliers_test = ((df_test[common_columns] < (Q1 - 1.5 * IQR)) | (df_test[common_columns] > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "# Step 6: Remove outliers (optional, you can choose to replace them as well)\n",
    "df_train_clean = df_train[~outliers_train.any(axis=1)]  # Removing rows with any outlier\n",
    "df_test_clean = df_test[~outliers_test.any(axis=1)]      # Removing rows with any outlier\n",
    "\n",
    "# Step 7: Save the cleaned datasets (if required)\n",
    "df_train_clean.to_csv('cleaned_train_dataset.txt', sep='\\t', index=False)\n",
    "df_test_clean.to_csv('cleaned_test_dataset.txt', sep='\\t', index=False)\n",
    "\n",
    "# Output the cleaned shape\n",
    "print(f\"Original Train Shape: {df_train.shape}, Cleaned Train Shape: {df_train_clean.shape}\")\n",
    "print(f\"Original Test Shape: {df_test.shape}, Cleaned Test Shape: {df_test_clean.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
